<p>I wrote this a few weeks back.</p>
<h2>On Vibe Coding</h2>
<p>This weekend I started throwing Claude Code at a 3rd-person shooter idea that I had started to implement in Godot engine manually. Initially progress was blindingly fast. Claude got the basic multiplayer aspect working in a couple prompts, which had been especially daunting to me. But as the weekend drew on, and my understanding of my own project dwindled, we started </p>
<p>to run into more and more blockers. Sometimes throwing out recent code and starting from scratch with a new prompt would get us through, but more and more it felt like I was just acting as Claude&#39;s eyes and ears as, unlike traditional web development, Claude can&#39;t easily test game code itself. And without a human&#39;s understanding, Claude was just digging itself into harder and harder to debug issues, where telling it to try again in a different way worked less and less.</p>
<p>The central paradox is that coding agents (esp. Claude Code in my experience) are definitely able to take a well defined task in a common framework and produce something of value, but it takes an experienced developer with good problem-domain and framework knowledge to iterate on a project using agents to avoid unmaintainable buggy spaghetti code. The developer also has to guard against going too fast and ending up with code they don&#39;t fully understand and thus be unable to unstick the bot when it inevitably can&#39;t dig itself out of its own complexity.</p>
<p>If you take someone with no coding experience, or even a developer, but one who is not familiar with the framework and problem-domain they are vibe-coding for, they are going to sooner or later, depending on their aptitude and caution, have the bot dig itself into a hole that the human doesn&#39;t have the tools or knowledge to dig out.</p>
<h2>So how can we best leverage coding agents?</h2>
<p>Firstly, give the agent a way to test its own code. Have it fire up the project via shell commands and read the error output itself. For visual elements, give it a way to take screenshots of what they&#39;re working on. This way, the agent will be productive for longer without human intervention. Agents and humans both need to run the code they write to find problems and bugs.</p>
<p>Developers should treat coding agents as &quot;junior&quot; developers, by giving them tasks that are focused, concrete, and limited in scope. We then have to be sure to thoroughly review and test the generated code.</p>
<p>Just as important, the developer must continue &quot;manually&quot; coding themselves, because without continuously pushing themselves as a code reviewer <em>and</em> coder, their skills will atrophy and they will end up like someone who doesn&#39;t code and fails to vibe-code their app idea because they eventually get bogged down by architectural and knowledge debt.</p>
<h2>Additional Reading</h2>
<p><a href="https://dylanbeattie.net/2025/04/11/the-problem-with-vibe-coding.html">https://dylanbeattie.net/2025/04/11/the-problem-with-vibe-coding.html</a>
<a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude">https://diwank.space/field-notes-from-shipping-real-code-with-claude</a>
<a href="https://blog.singleton.io/posts/2025-06-14-coding-agents-cross-a-chasm/">https://blog.singleton.io/posts/2025-06-14-coding-agents-cross-a-chasm/</a></p>
